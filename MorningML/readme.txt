
Done:
- Introduced input layer
- Now we only define the number of units in layer
- WX + B implemented for inner product
- NNActivation layer introduced
  > to check the consistency in dimension of input layer

To do:
- do the sigmoid for sigma/relu, etc.
- write the Loss function
- Do the backward pass

